/*
 * cache.cpp
 *
 *  Created on: Nov 11, 2015
 *      Author: uzleo
 */




#include "cache.hpp"

// to clean up allocated stuff globally for the whole class
std::vector< cache::req_extension * > cache::req_extension::req_extension_clones;

// constructor
cache::cache(sc_core::sc_module_name name, const char *logfile, uint32_t num_masters, uint32_t size, uint32_t block_size, uint32_t num_ways, bool write_back, bool write_allocate, cache::eviction_policy evict_policy, uint32_t level)
	:	sc_module(name),
		m_trans(),																// transaction object
		m_num_upstream_masters(num_masters),									// total upstream masters (num of children i.e num of caches that share this cache)
		m_block_size(block_size),												// total cache size
		m_num_of_sets(size/(block_size*num_ways)),								// total number of sets
		m_num_of_ways(num_ways),												// total number of ways
		m_write_back(write_back),												// write back enable/disable
		m_write_allocate(write_allocate),										// write allocate enable/disable
		m_evict_policy(evict_policy),											// cache block replacement policy
		m_log(false),															// enable logging (on text files referred to by logfile
		m_level(level),															// cache hierarchy level (first level cache (L1) is 1)
		m_current_set(0),														// set for current request
		m_current_tag(0),														// tag for current request
		m_current_blockAddr(0),													// cache block address for current request
		m_current_way(0),														// cache way for current request
		m_current_delay(sc_core::sc_time(0, sc_core::SC_NS)),					// delay object for current request
		m_isocket_d("m_isocket_d")												// initiator socket to downstream slave
{
	// ensuring that set bits <= mem_size_bits.....sort of a corner case but not expected to occur in real cache organizations
	// if this does occur though then cache would be inefficient as it will have slots for memory words which actually dont exist in memory......cache will be bigger than memory!
	assert(log2((double) m_num_of_sets) <= log2((double) MEM_SIZE));

	// cache blocks structure
	m_blocks.resize(m_num_of_sets);
	for (uint32_t i=0; i<m_num_of_sets; i++) {
		m_blocks[i].resize(m_num_of_ways);
		for (uint32_t j=0; j<m_num_of_ways; j++) {
			m_blocks[i][j].state = cache_block::I;
		}
	}

	// req type indicator
	m_ext = new req_extension();
	m_trans.set_extension(m_ext);

	// target sockets for upstream masters
	m_tsocket_d = new tlm_utils::simple_target_socket< cache >[m_num_upstream_masters];
	for (uint32_t i=0; i<m_num_upstream_masters; i++) {
		m_tsocket_d[i].register_b_transport(this, &cache::b_transport);
	}

	// initiator sockets to upstream slaves (for back invalidation)
	if (m_level > 1) {		// not first level cache
		m_isocket_u = new tlm_utils::simple_initiator_socket< cache >[m_num_upstream_masters];
	}

	// target sockets for downstream masters (for back invalidation)
	if (m_level < LLC_LEVEL) {			// not last level cache
		m_tsocket_u = new tlm_utils::simple_target_socket< cache >;
		m_tsocket_u->register_b_transport(this, &cache::b_transport);
	}

	// logging
	m_fid = fopen(logfile, "w+");
	assert(m_fid);
	fprintf(m_fid, "CACHE_CONFIG \r\n");
	fprintf(m_fid, "total_cache_size:%dB", size);
	fprintf(m_fid, "..cache_block_size:%dB", m_block_size);
	fprintf(m_fid, "..num_of_sets:%d", m_num_of_sets);
	fprintf(m_fid, "..num_of_ways:%d", m_num_of_ways);
	fprintf(m_fid, "..num_of_children:%d", m_num_upstream_masters);
	fprintf(m_fid, "..write_back:%d", m_write_back);
	fprintf(m_fid, "..write_allocate:%d", m_write_allocate);
	fprintf(m_fid, "..evict_policy:%d\r\n\r\n", m_evict_policy);
	fflush(m_fid);
}


// deleting allocated stuff to avoid memory leaks
cache::~cache() {
	delete[] m_tsocket_d;
	if (m_level > 1) {
		delete[] m_isocket_u;
	}
	if (m_level < LLC_LEVEL) {
		delete m_tsocket_u;
	}
	delete m_ext;
	fclose(m_fid);
	delete m_fid;
}



// enable logging
void cache::do_logging() {
	m_log = true;
}


// port interface method
void cache::b_transport(tlm::tlm_generic_payload &trans, sc_core::sc_time &delay) {
	uint64_t req_addr = trans.get_address();
	tlm::tlm_command cmd = trans.get_command();
	req_extension *ext;
	trans.get_extension(ext);
	req_extension::req_type type = ext->m_type;
	bool evict_needed;

	// update for current request (this only works if there are no outstanding request like in LT modeling)
	m_current_blockAddr = (req_addr/m_block_size)*m_block_size;
	m_current_set = (m_current_blockAddr >> (uint32_t)(log2((double)WORD_SIZE) + log2((double)m_block_size/WORD_SIZE))) & ((1 << (uint32_t)log2((double)m_num_of_sets)) - 1);
	m_current_tag = (req_addr >> (uint32_t)(log2((double)WORD_SIZE) + log2((double)m_block_size/WORD_SIZE) + log2((double) m_num_of_sets)));
	m_current_delay = delay;

	if (cache_lookup(evict_needed, m_current_way)) {
		// cache hit
		if (type == req_extension::NORMAL) {
			// normal request
			process_normal_request(trans);
		} else {
			// special request
			process_special_request(type);
		}
	} else {
		// cache miss
		assert(type == req_extension::NORMAL);			// the special request shouldn't result in a miss
		process_miss(trans, evict_needed);
	}

	if (m_log) {
		print_cache_set(set);
	}

	trans.set_response_status(tlm::TLM_OK_RESPONSE);
}


// performs a cache lookup for the passed req_addr and returns true if hit else false..also assert evict_needed incase if eviction needed
// way_free logic:
//		-- incase of hit, it is set to found way
//		-- incase of miss, it is set to the way which is going to be refilled with new block from downstream
bool cache::cache_lookup(bool &evict_needed, uint32_t &way_free) {
	evict_needed = true;

	for (uint32_t i=0; i<m_num_of_ways; i++) {
		if (m_blocks[m_current_set][i].state != cache_block::I) {
			if (m_blocks[m_current_set][i].tag == m_current_tag) {
				evict_needed = false;
				way_free = i;
				return true;
			}
		} else {
			evict_needed = false;
			way_free = i;
		}
	}

	if (evict_needed) {
		way_free = find_way_evict();
	}

	return false;
}


// handle normal hit request
void cache::process_normal_request(tlm::tlm_generic_payload &trans) {
	// logging
	if (m_log) 	{
		fprintf(m_fid, "cache hit for 0x%08x", (uint32_t)trans.get_address());
		fprintf(m_fid, "..tag=0x%08x", (uint32_t)m_current_tag);
		fprintf(m_fid, "..set=%d\r\n", m_current_set);
		fflush(m_fid);			// disable this when not debugging
	}

	tlm::tlm_command cmd = trans.get_command();
	cache_block::cache_block_state *state = &m_blocks[m_current_set][m_current_way].state;

	if (cmd == tlm::TLM_WRITE_COMMAND) {
		if (m_write_back) {
			if (*state == cache_block::S) {
				*state = cache_block::M;
				if (m_level < LLC_LEVEL) {
					// in case of going to M state, notifying the downstream caches to update their block states (from S to MBS) via special request
					m_ext->m_type = req_extension::M_UPDATE;
					send_request(true);
				}
			}
		} else {
			// writing through downstream
			m_trans.set_write();
			m_ext->m_type = req_extension::NORMAL;
			send_request(true);
		}
	} else {
		// todo: delay timing for reading cache block
	}
}


void cache::process_special_request(req_extension::req_type type) {
	// logging
	if (m_log) {
		// TODO
	}

	cache_block::cache_block_state *state = &m_blocks[m_current_set][m_current_way].state;

	switch(type) {
		case req_extension::M_UPDATE : {
			// request from child that it is now caching this block in M state
			assert(*state != cache_block::I);				// can't be in invalid state (strictly inclusive caches)
			if (*state == cache_block::S) {
				*state = cache_block::MBS;					// TODO: Does MBS make sense for write through caches as well???
			} else if (*state == cache_block::M) {
				assert(m_write_back);						// this has to be writeback cache
				*state = cache_block::MBS;
			}
			// forwarding this special request to further downstream caches if present
			if (m_level < LLC_LEVEL) {
				m_ext->m_type = req_extension::M_UPDATE;
				send_request(true);
			}
			break;
		}
		case req_extension::BACK_INVALIDATE : {
			// request from parent for back-invalidation
			if (*state == cache_block::S) {
				*state = cache_block::I;
			} else if (*state == cache_block::M) {
				assert(m_level < LLC_LEVEL);			// this can't be last level
				*state = cache_block::I;
				// writing back downstream
				m_ext->m_type = req_extension::WB_UPDATE;
				send_request(true);
			}
			// else no need to do anything if (state = MBS) path as it will be invalidated on WB_UPDATE path

			// if there are upstream caches then sending back invalidation to them as well
			if (m_level > 1) {
				m_ext->m_type = req_extension::BACK_INVALIDATE;
				send_request(false);
			}
			break;
		}
		case req_extension::WB_UPDATE : {
			// request from child for WB update
			if (*state == cache_block::I) {
				// this is the cache that initiated the back-invalidation path
				// writing through downstream
				m_trans.set_write();
				m_ext->m_type = req_extension::NORMAL;
				send_request(true);
			} else {
				assert(*state == cache_block::MBS);				// this has to be in MBS state........TODO: right now for write-through caches as well..can MBS for write-through cache be avoided???
				assert(m_level < LLC_LEVEL);					// this can't be last level
				*state = cache_block::I;
				// writing back downstream
				m_ext->m_type = req_extension::WB_UPDATE;
				send_request(true);
			}
			break;
		}
		case req_extension::NORMAL : {
			assert(0);			// shouldn't come here
			break;
		}
		default : {
			assert(0);			// shouldn't come here
			break;
		}
	}
}


void cache::process_miss(tlm::tlm_generic_payload &trans, bool evict_needed) {
	if (m_log) {
		fprintf(m_fid, "cache miss for 0x%08x", (uint32_t)trans.get_address());
		fprintf(m_fid, "..tag=0x%08x", (uint32_t)m_current_tag);
		fprintf(m_fid, "...set=%d\r\n", m_current_set);
		fflush(m_fid);			//TODO: disable this after debugging
	}

	if (evict_needed) {
		do_eviction();
	}

	// fetch the block from downstream
	m_trans.set_read();
	m_trans.set_address(block_addr);
	send_request(true, delay);

	// update the state of cache block
	update_block_state(req_addr, cmd, way_free, delay);
}


void cache::do_eviction() {
	cache_block::cache_block_state *state = &m_blocks[m_current_set][m_current_way].state;
	*state = cache_block::I;

	// BACK INVALIDATION
	if (m_level > 1) {			// there are upstream caches
		m_ext->m_type = req_extension::BACK_INVALIDATE;
		send_request(false);
	}

	// incase this block is in 'S' then no writeback is needed
	// incase this block is in 'MBS' then this means that this block is being cached somewhere upstream in 'M' state which would have been written back via the back-invalidation path above so no writeback is needed
	// incase this block is in 'M' then write back is needed downstream
	/*if (*state != cache_block::S) {
		*state = cache_block::I;
		if (*state==cache_block::M) {
			assert(m_write_back);
			// writing through downstream..if a cache exists on the downstream path then it can update the status of its block (MBS) to M
			m_trans.set_write();
			m_trans.set_address(block_addr);
			send_request(true, delay);
		}
	}*/
}


void cache::print_cache_set(uint32_t set) {
	for (uint32_t x=0; x<m_num_of_ways; x++) {
		fprintf(m_fid, "way[%d]..", x);
		fprintf(m_fid, "state=%d..", m_blocks[set][x].state);
		fprintf(m_fid, "tag=0x%08x..", (uint32_t)m_blocks[set][x].tag);
		fprintf(m_fid, "lru=%d\r\n", m_blocks[set][x].evict_tag);
	}
	fprintf(m_fid, "------------------\r\n");
	fflush(m_fid);			//TODO: disable this after debugging
}


void cache::update_block_state(uint64_t req_addr, tlm::tlm_command cmd, uint32_t way_free, sc_core::sc_time &delay) {
	uint64_t block_addr, tag;
	uint32_t set;
	blkAddr_set_tag(req_addr, block_addr, set, tag);

	assert(m_blocks[set][way_free].state == cache_block::I);
	m_blocks[set][way_free].tag = tag;
	if (cmd == tlm::TLM_WRITE_COMMAND) {
		if (m_write_back) {
			m_blocks[set][way_free].state = cache_block::M;
			m_ext->m_type = req_extension::M_UPDATE;
			m_trans.set_address(block_addr);
			send_request(true, delay);
		} else {
			m_blocks[set][way_free].state = cache_block::S;
			// writing through downstream
			m_trans.set_write();
			m_trans.set_address(block_addr);
			send_request(true, delay);
		}
	} else {
		m_blocks[set][way_free].state = cache_block::S;
	}

	// eviction policy stuff for this newly cached block
	if (m_evict_policy == LRU) {
		for (uint32_t j=0; j<m_num_of_ways; j++) {
			if (way_free!=j && m_blocks[set][j].state!=cache_block::I) {
				m_blocks[set][j].evict_tag++;
			}
			assert(m_blocks[set][way_free].evict_tag <= m_num_of_ways);
		}
		m_blocks[set][way_free].evict_tag = 1;
	} else {
		m_blocks[set][way_free].evict_tag = 0;
	}
}





uint32_t cache::find_way_evict() {
	int way_free = -1;

	switch(m_evict_policy) {
		case LRU:
		{
			for (uint32_t j=0; j<m_num_of_ways; j++) {
				if (m_blocks[m_current_set][j].evict_tag == m_num_of_ways) {
					way_free = j;
				}
			}
			assert(way_free != -1);
			break;
		}
		case LFU:
		{
			uint64_t tmp = m_blocks[m_current_set][0].evict_tag;
			way_free = 0;
			for (uint32_t j=1; j<m_num_of_ways; j++) {
				if (tmp > m_blocks[m_current_set][j].evict_tag) {
					tmp = m_blocks[m_current_set][j].evict_tag;
					way_free = j;
				}
			}
			break;
		}
		case FIFO:
		{
			//TODO
			break;
		}
		case RAND:
		{
			way_free = rand()%4;
			break;
		}
		default:
		{
			assert(0);
		}
	}

	return (uint32_t)way_free;
}


// request transport mechanism
void cache::send_request(bool downstream) {
	m_trans.set_address(m_current_blockAddr);
	m_trans.set_response_status(tlm::TLM_INCOMPLETE_RESPONSE);
	if (downstream) {
		// downstream 'true' will send the request downstream
		m_isocket_d->b_transport(m_trans, m_current_delay);
	} else {
		// for downstream 'false' will send request to all upstream masters
		for (uint32_t i=0; i<m_num_upstream_masters; i++) {
			m_isocket_u[i]->b_transport(m_trans, m_current_delay);
		}
	}
	assert(m_trans.get_response_status() == tlm::TLM_OK_RESPONSE);
}



//TODO: write-allocate
//TODO: fifo eviction policy
//TODO: model timing














