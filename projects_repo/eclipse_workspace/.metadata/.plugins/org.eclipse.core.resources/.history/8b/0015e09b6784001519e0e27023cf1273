/*
 * cache.cpp
 *
 *  Created on: Oct 20, 2015
 *      Author: uzair
 */




#include "cache.hpp"
#include <math.h>
#include <algorithm>

cache::cache(sc_core::sc_module_name name, uint32_t total_cache_size, uint32_t cache_line_size, uint32_t num_of_ways, bool write_back, bool write_allocate, cache::eviction_policy evict_pol)
	:	sc_module(name),
		m_total_cache_size(total_cache_size),
		m_cache_line_size(cache_line_size),
		m_num_of_sets(total_cache_size/(cache_line_size*num_of_ways)),
		m_num_of_ways(num_of_ways),
		m_write_back(write_back),
		m_write_allocate(write_allocate),
		m_evict(evict_pol)
{
	m_cache_lines.resize(m_num_of_sets);
	for (uint32_t i=0; i<m_num_of_sets; i++) {
		m_cache_lines[i].resize(m_num_of_ways);
		for (uint32_t j=0; j<m_num_of_ways; j++) {
			m_cache_lines[i][j].valid = false;
		}
	}

	// ensuring that set bits <= mem_size_bits.....sort of a corner case but not expected to occur in real cache organizations
	// if this does occur though then cache would be inefficient as it will have slots for memory words which actually dont exist in memory......cache will be bigger than memory!
	assert(log2((double) m_num_of_sets) <= log2((double) MEM_SIZE));


	printf("CACHE_CONFIG \r\n");
	std::cout << name;
	printf("..total_cache_size:%dB", m_total_cache_size);
	printf("..cache_block_size:%dB", m_cache_line_size);
	printf("..num_of_sets:%d", m_num_of_sets);
	printf("..num_of_ways:%d", m_num_of_ways);
	printf("..write_back:%d", m_write_back);
	printf("..write_allocate:%d", m_write_allocate);
	printf("..evict_policy:%d\r\n\r\n", m_evict);

}

void cache::update(tlm::tlm_generic_payload &payload, sc_core::sc_time &delay) {

	addr_t addr = (payload.get_address()/m_cache_line_size)*m_cache_line_size;			// rounded to cache-block address
	tlm::tlm_command cmd = payload.get_command();

	int way_free = -1;

	// finding out the cache set for this address for lookup
	uint32_t set = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE))) & ((1 << (uint32_t) log2((double)m_num_of_sets)) - 1);
	// as well as the tag
	addr_t tag = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE) + log2((double) m_num_of_sets)));

	delay += m_lookup_delay;
	// cache lookup for the found tag
	for (uint32_t i=0; i<m_num_of_ways; i++) {
		if (m_cache_lines[set][i].state != cache_line::I) {			// checking for valid cache blocks (M or S)
			if (m_cache_lines[set][i].tag == tag) {
				printf("(%s)..", name());
				printf("cache hit for 0x%08x", payload.get_address());
				printf("..tag=0x%08x", tag);
				printf("...set=%d\r\n", set);

				if (cmd == tlm::TLM_WRITE_COMMAND) {	// write hit
					if (!m_child) {
						// this is the first level cache
						if (m_write_back) {
							// for the lowest level cache, marking this block as dirty incase of write hit and write-back policy being used
							delay += m_write_cache_delay;
							if (m_cache_lines[set][i].state == cache_line::S) {
								// if this cache is now modifying this block then inform its parent cache so it can change state accordingly
								// TODO: m_parent->update_state(...);
								// TODO: the update_state should update the delay as well!!
							}
							m_cache_lines[set][i].state = cache_line::M;
						} else {
							//delay += m_downstream_cacheblock_delay;
							// for write-through policy, we assume that there are write buffers b/w cache and higher level cache/memory module so as to hide the write word delay to this cache
							// hence delay not updated here
						}
					} else {
						// write miss in l1 cache but hit in higher level cache hierarchy
						// reading the block (most updated block in cache hierarchy) for transferring to lower level cache
						delay += m_read_cache_delay;
						// going to MODIFIED_BUT_STALE state as now the current data will be in lower level cache
						m_cache_lines[set][i].state = cache_line::MBS;
						// setting the block state for all parents upto memory to be MBS as well
						// TODO: m_parent->update_state(...);
						// TODO: the update_state should update the delay as well!!
					}
				} else if (cmd == tlm::TLM_READ_COMMAND) {	// read hit
					// for read case we dont need to care for whether it is a hit in lowest level cache or a higher level cache as no states are being updated
					delay += m_read_cache_delay;
				}

				// for eviction policy management
				switch(m_evict) {
					case LRU:
						for (uint32_t j=0; j<m_num_of_ways; j++) {
							if (i!=j && m_cache_lines[set][j].state!=cache_line::I && m_cache_lines[set][j].evict_tag<=m_cache_lines[set][i].evict_tag) {
								m_cache_lines[set][j].evict_tag++;
							}
							assert(m_cache_lines[set][j].evict_tag <= m_num_of_ways);
						}
						m_cache_lines[set][i].evict_tag = 1;
						break;
					case LFU:
						m_cache_lines[set][i].evict_tag++;
						break;
					case RAND:
						break;
					default:
						assert(0);
				}

				for (uint32_t x=0; x<m_num_of_ways; x++) {
					std::cout<<name()<<"_";
					printf("way[%d]..", x);
					printf("state=%d..", m_cache_lines[set][x].state);
					printf("tag=0x%08x..", m_cache_lines[set][x].tag);
					printf("lru=%d\r\n", m_cache_lines[set][x].evict_tag);
				}

				return;
			}
		} else {
			way_free = i;
		}
	}
	printf("(%s)..", name());
	printf("cache miss for 0x%08x", payload.get_address());
	printf("..tag=0x%08x", tag);
	printf("...set=%d\r\n", set);

	if (cmd == tlm::TLM_WRITE_COMMAND && !m_write_allocate) {
		// if write miss and write-no-allocate policy being selected then not updating anything in cache for this case...just modeling delay for writing the word into downstream memory module

		delay += m_downstream_cacheblock_delay;
	} else {
		if (way_free == -1) {
			// there is no free way available for caching new block so have to evict one
			// finding the way to be evicted based on eviction policy
			uint64_t tmp = 0;
			switch(m_evict) {
				case LRU:
					for (uint32_t j=0; j<m_num_of_ways; j++) {
						if (m_cache_lines[set][j].evict_tag == m_num_of_ways) {
							way_free = j;
						}
					}
					break;
				case LFU:
					tmp = m_cache_lines[set][0].evict_tag;
					way_free = 0;
					for (uint32_t j=1; j<m_num_of_ways; j++) {
						if (tmp > m_cache_lines[set][j].evict_tag) {
							tmp = m_cache_lines[set][j].evict_tag;
							way_free = j;
						}
					}
					break;
				case RAND:
					way_free = rand()%4;
					break;
				default:
					assert(0);
			}

			// evicting the cache-block in way_free (found above)
			// if the block is dirty then writing back to higher level cache/memory module (any block cached in lower level cache must be in its parent caches by inclusion)
			if (m_write_back && m_cache_lines[set][way_free].state==cache_line::M) {
				delay += m_downstream_cacheblock_delay;

				// notify the parent cache so that it can mark its block dirty with new written data
				if (m_parent) {
					/*m_parent->handle_writeback(payload.get_address());
					delay += WRITEBACK_NOTIFICATION_DELAY;*/
					// TODO: m_parent->update_state(...);
					// TODO: the update_state should update the delay as well!!
				}

				// notifying every child cache that this block is going to be evicted so invalidation of this block is required in case if they are being cached
				if (m_child) {
					for (uint32_t i=0; i<m_child->size(); i++) {
						(*m_child)[i]->handle_invalidation_request(payload.get_address(), delay);
					}
				}
			}
		}

		// requesting cache block from higher level cache/memory module
		if (m_parent) {
			// for lower level caches, requesting the block from next higher cache in hierarchy
			m_parent->update(payload, delay);
		}
		delay += m_upstream_cacheblock_delay;			// cache block transfer delay
		delay += m_write_cache_delay;

		// caching this new block
		m_cache_lines[set][way_free].valid = true;
		m_cache_lines[set][way_free].dirty = false;
		m_cache_lines[set][way_free].tag = tag;
		if (m_evict == LRU) {
			for (uint32_t j=0; j<m_num_of_ways; j++) {
				if ((uint32_t)way_free!=j && m_cache_lines[set][j].valid==true) {
					m_cache_lines[set][j].evict_tag++;
				}
				assert(m_cache_lines[set][j].evict_tag <= m_num_of_ways);
			}
			m_cache_lines[set][way_free].evict_tag = 1;
		} else {
			m_cache_lines[set][way_free].evict_tag = 0;
		}
		if (cmd == tlm::TLM_WRITE_COMMAND) {					// write miss
			// only the lowest level cache would update this block and mark it as dirty
			if (!m_child) {
				if (m_write_back) {
					m_cache_lines[set][way_free].dirty = true;
					delay += m_write_cache_delay;
				} else {
					//delay += m_downstream_cacheblock_delay;
					// for write-through policy, we assume that there are write buffers b/w cache and higher level cache/memory module so as to hide the write word delay to this cache
					// hence delay not updated here
				}
			} else {
				delay += m_read_cache_delay;
			}
		} else if (cmd == tlm::TLM_READ_COMMAND) {			// read miss
			delay += m_read_cache_delay;
		}
	}

	for (uint32_t x=0; x<m_num_of_ways; x++) {
		std::cout<<name()<<"_";
		printf("way[%d]..", x);
		printf("valid=%d..", m_cache_lines[set][x].valid);
		printf("dirty=%d..", m_cache_lines[set][x].dirty);
		printf("tag=0x%08x..", m_cache_lines[set][x].tag);
		printf("lru=%d\r\n", m_cache_lines[set][x].evict_tag);
	}
}




void cache::set_parent(cache *parent) {
	m_parent = parent;
}

void cache::set_children(std::vector< cache * > *child) {
	m_child = child;
}

void cache::set_delays(sc_core::sc_time upstream, sc_core::sc_time downstream, sc_core::sc_time lookup, sc_core::sc_time write, sc_core::sc_time read) {
	m_lookup_delay = lookup;
	m_write_cache_delay = write;
	m_read_cache_delay = read;
	m_upstream_cacheblock_delay = upstream;
	m_downstream_cacheblock_delay = downstream;
}

void cache::handle_invalidation_request(addr_t req_addr, sc_core::sc_time &delay) {
	delay += BACKINVALIDATION_NOTIFICATION_DELAY;
	// request from the parent to invalidate this cache block if it is being cached here(inclusion property)..this will be called when the parent is going to evict this block

	addr_t addr = (req_addr/m_cache_line_size)*m_cache_line_size;			// rounded to cache-block address
	// finding out the cache set for this address for lookup
	uint32_t set = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE))) & ((1 << (uint32_t) log2((double)m_num_of_sets)) - 1);
	// as well as the tag
	addr_t tag = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE) + log2((double) m_num_of_sets)));

	for (uint32_t i=0; i<m_num_of_ways; i++) {
		if (m_cache_lines[set][i].valid && m_cache_lines[set][i].tag==tag) {
			m_cache_lines[set][i].valid = false;
			// lru stuff!!
			if (m_evict == LRU) {
				for (uint32_t j=0; j<m_num_of_ways; j++) {
					if ((uint32_t)i!=j && m_cache_lines[set][j].valid==true && m_cache_lines[set][j].evict_tag>m_cache_lines[set][i].evict_tag) {
						m_cache_lines[set][j].evict_tag--;
					}
					assert(m_cache_lines[set][j].evict_tag <= m_num_of_ways);
				}
				m_cache_lines[set][i].evict_tag = m_num_of_ways;
			}
			break;
		}
	}

	// notifying about invalidating this block in lower level caches as well
	if (m_child) {
		for (uint32_t i=0; i<m_child->size(); i++) {
			(*m_child)[i]->handle_invalidation_request(req_addr, delay);
		}
	}

	//TODO: in the lowest level cache, if the block to be invalidated is dirty and cache is write-back then have to write that block to memory as well OR writeback that blcok to next higher cache......what if blocksize in higher cache > blocksize in this cache then have to evict multiple blocks in this scenario, how to do that????........what to do if this is not the lowest level cache with the dirty block??
}


void cache::handle_writeback(addr_t req_addr) {
	// writeback request from the child...marking this block dirty now in this cache

	addr_t addr = (req_addr/m_cache_line_size)*m_cache_line_size;			// rounded to cache-block address
	// finding out the cache set for this address for lookup
	uint32_t set = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE))) & ((1 << (uint32_t) log2((double)m_num_of_sets)) - 1);
	// as well as the tag
	addr_t tag = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE) + log2((double) m_num_of_sets)));

	for (uint32_t i=0; i<m_num_of_ways; i++) {
		if (m_cache_lines[set][i].valid && m_cache_lines[set][i].tag==tag) {
			assert(m_cache_lines[set][i].dirty == false);
			m_cache_lines[set][i].dirty = true;
			break;
		}
	}
}



// TODO: model delays for cache block requests from lower level cache to higher level in cache hierarchy







// TODO: FIFO replacement policy cache line
// TODO: review timing modeling


// sofar no support for cache coherence
// to avoid cache coherency problems in child caches sharing a common parent like split l1 (i/d) cache with unified l2 cache, the child caches (l1 i/d) should be made write through so that childs are not caching the same block with dirty state simultaneously with their own data









