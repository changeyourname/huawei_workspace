/*
 * cache.cpp
 *
 *  Created on: Oct 20, 2015
 *      Author: uzair
 */




#include "cache.hpp"
#include <math.h>
#include <algorithm>

cache::cache(sc_core::sc_module_name name, uint32_t total_cache_size, uint32_t cache_line_size, uint32_t num_of_ways, bool write_back, bool write_allocate, cache::eviction_policy evict_pol)
	:	sc_module(name),
		m_total_cache_size(total_cache_size),
		m_cache_line_size(cache_line_size),
		m_num_of_sets(total_cache_size/(cache_line_size*num_of_ways)),
		m_num_of_ways(num_of_ways),
		m_write_back(write_back),
		m_write_allocate(write_allocate),
		m_evict(evict_pol)
{
	m_cache_lines.resize(m_num_of_sets);
	for (uint32_t i=0; i<m_num_of_sets; i++) {
		m_cache_lines[i].resize(m_num_of_ways);
		for (uint32_t j=0; j<m_num_of_ways; j++) {
			m_cache_lines[i][j].state = cache_line::I;
		}
	}

	// ensuring that set bits <= mem_size_bits.....sort of a corner case but not expected to occur in real cache organizations
	// if this does occur though then cache would be inefficient as it will have slots for memory words which actually dont exist in memory......cache will be bigger than memory!
	assert(log2((double) m_num_of_sets) <= log2((double) MEM_SIZE));


	printf("CACHE_CONFIG \r\n");
	std::cout << name;
	printf("..total_cache_size:%dB", m_total_cache_size);
	printf("..cache_block_size:%dB", m_cache_line_size);
	printf("..num_of_sets:%d", m_num_of_sets);
	printf("..num_of_ways:%d", m_num_of_ways);
	printf("..write_back:%d", m_write_back);
	printf("..write_allocate:%d", m_write_allocate);
	printf("..evict_policy:%d\r\n\r\n", m_evict);

}

void cache::update(tlm::tlm_generic_payload &payload, sc_core::sc_time &delay, bool write_through) {

	int way_free = -1;
	tlm::tlm_command cmd = payload.get_command();

	addr_t addr = (payload.get_address()/m_cache_line_size)*m_cache_line_size;			// rounded to cache-block address
	// finding out the cache set for this address for lookup
	uint32_t set = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE))) & ((1 << (uint32_t) log2((double)m_num_of_sets)) - 1);
	// as well as the tag
	addr_t tag = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE) + log2((double) m_num_of_sets)));

	// cache lookup for the found tag
	delay += m_lookup_delay;
	for (uint32_t i=0; i<m_num_of_ways; i++) {
		if (m_cache_lines[set][i].state != cache_line::I) {			// checking for valid cache blocks (M or S)
			if (m_cache_lines[set][i].tag == tag) {
				if (!write_through) {
					printf("(%s)..", name());
					printf("cache hit for 0x%08x", payload.get_address());
					printf("..tag=0x%08x", tag);
					printf("...set=%d\r\n", set);
				}


				if (cmd == tlm::TLM_WRITE_COMMAND) {	// write hit
					// checking to see if the request is from child for write-through/write-back
					if (write_through) {
						if (m_cache_lines[set][i].state==cache_line::S || m_cache_lines[set][i].state==cache_line::MBS) {
							// if this cache is write-back then this cache has the most recent data now
							if (m_write_back) {
								// notifying the parent caches that this cache has the most updated data now
								if (m_parent && m_cache_lines[set][i].state==cache_line::S) {
									m_parent->update_state(1, payload.get_address(), delay);
								}
								delay += m_write_cache_delay;
								m_cache_lines[set][i].state = cache_line::M;
							} else {
								// writing through to next higher level
								if (m_parent) {
									m_parent->update(payload, delay, true);
									delay += WRITE_THROUGH_DELAY;
								} else {
									// writing to memory
									delay += m_downstream_cacheblock_delay;
								}
							}
						} else {
							// this block can't be in invalid or in modified state
							assert(0);
						}
						return;
					}

					if (!m_child) {
						// this is the first level cache
						if (m_write_back) {
							// for the lowest level cache, marking this block as dirty (state->M) incase of write hit and write-back policy being used incase it is already not have it modified
							delay += m_write_cache_delay;
							if (m_cache_lines[set][i].state == cache_line::S) {
								// notifying the parent caches that this cache has the most updated data now
								if (m_parent) {
									m_parent->update_state(1, payload.get_address(), delay);
								}
								m_cache_lines[set][i].state = cache_line::M;
							}
						} else {
							// writing through to next higher level
							if (m_parent) {
								m_parent->update(payload, delay, true);
							}
						}
					} else {
						// write miss in l1 cache but hit in higher level cache hierarchy
						// reading the block (most updated block in cache hierarchy) for transferring to lower level cache
						delay += m_read_cache_delay;
					}
				} else if (cmd == tlm::TLM_READ_COMMAND) {	// read hit
					// for read case we dont need to care for whether it is a hit in lowest level cache or a higher level cache as no states are being updated
					delay += m_read_cache_delay;
				}

				// for eviction policy management
				switch(m_evict) {
					case LRU:
						for (uint32_t j=0; j<m_num_of_ways; j++) {
							if (i!=j && m_cache_lines[set][j].state!=cache_line::I && m_cache_lines[set][j].evict_tag<=m_cache_lines[set][i].evict_tag) {
								m_cache_lines[set][j].evict_tag++;
							}
							assert(m_cache_lines[set][j].evict_tag <= m_num_of_ways);
						}
						m_cache_lines[set][i].evict_tag = 1;
						break;
					case LFU:
						m_cache_lines[set][i].evict_tag++;
						break;
					case RAND:
						break;
					default:
						assert(0);
				}

				for (uint32_t x=0; x<m_num_of_ways; x++) {
					std::cout<<name()<<"_";
					printf("way[%d]..", x);
					printf("state=%d..", m_cache_lines[set][x].state);
					printf("tag=0x%08x..", m_cache_lines[set][x].tag);
					printf("lru=%d\r\n", m_cache_lines[set][x].evict_tag);
				}

				return;
			}
		} else {
			way_free = i;
		}
	}
	printf("(%s)..", name());
	printf("cache miss for 0x%08x", payload.get_address());
	printf("..tag=0x%08x", tag);
	printf("...set=%d\r\n", set);

	if (cmd == tlm::TLM_WRITE_COMMAND && !m_write_allocate) {
		// if write miss and write-no-allocate policy being selected then not updating anything in cache for this case...just modeling delay for writing the word into downstream memory module

		delay += m_downstream_cacheblock_delay;
	} else {
		if (way_free == -1) {
			// there is no free way available for caching new block so have to evict one
			// finding the way to be evicted based on eviction policy
			uint64_t tmp = 0;
			switch(m_evict) {
				case LRU:
					for (uint32_t j=0; j<m_num_of_ways; j++) {
						if (m_cache_lines[set][j].evict_tag == m_num_of_ways) {
							way_free = j;
						}
					}
					break;
				case LFU:
					tmp = m_cache_lines[set][0].evict_tag;
					way_free = 0;
					for (uint32_t j=1; j<m_num_of_ways; j++) {
						if (tmp > m_cache_lines[set][j].evict_tag) {
							tmp = m_cache_lines[set][j].evict_tag;
							way_free = j;
						}
					}
					break;
				case RAND:
					way_free = rand()%4;
					break;
				default:
					assert(0);
			}

			// evicting the cache-block in way_free (found above)
			// if the block is dirty then writing back to higher level cache/memory module (any block cached in lower level cache must be in its parent caches by inclusion)
			if (m_write_back && m_cache_lines[set][way_free].state!=cache_line::S) {
				if (m_parent) {
					// writing through to next next higher level
					m_parent->update(payload, delay, true);
				} else {
					// this is the highest level cache so writing to memory
					delay += m_downstream_cacheblock_delay;
				}

				// notifying every child cache that this block is going to be evicted so invalidation of this block is required in case if they are being cached (BackInvalidation for maintaining strict inclusiveness)
				if (m_child) {
					for (uint32_t i=0; i<m_child->size(); i++) {
						(*m_child)[i]->update_state(2, payload.get_address(), delay);
					}
				}
			}
		}

		// requesting referred cache block from higher level cache/memory module
		if (m_parent) {
			// for lower level caches, requesting the block from next higher cache in hierarchy
			m_parent->update(payload, delay);
		}
		delay += m_upstream_cacheblock_delay;			// cache block transfer delay
		delay += m_write_cache_delay;

		// caching this new block
		m_cache_lines[set][way_free].tag = tag;
		if (!m_child) {
			// for lowest level cache
			if (cmd == tlm::TLM_WRITE_COMMAND) {
				delay += m_write_cache_delay;
				if (m_write_back) {
					m_cache_lines[set][way_free].state = cache_line::M;
					// notifying the parent caches that this cache has the most updated data now
					if (m_parent) {
						m_parent->update_state(1, payload.get_address(), delay);
					}
				} else {
					m_cache_lines[set][way_free].state = cache_line::S;
					// writing through to next higher level
					if (m_parent) {
						m_parent->update(payload, delay, true);
					}
				}
			}
		} else {
			// for higher level cache, simply read the cache block and pass onto the next lower cache level (child)
			m_cache_lines[set][way_free].state = cache_line::S;
			delay += m_read_cache_delay;
		}
		// eviction policy stuff
		if (m_evict == LRU) {
			for (uint32_t j=0; j<m_num_of_ways; j++) {
				if ((uint32_t)way_free!=j && m_cache_lines[set][j].state!=cache_line::I) {
					m_cache_lines[set][j].evict_tag++;
				}
				assert(m_cache_lines[set][j].evict_tag <= m_num_of_ways);
			}
			m_cache_lines[set][way_free].evict_tag = 1;
		} else {
			m_cache_lines[set][way_free].evict_tag = 0;
		}
	}

	for (uint32_t x=0; x<m_num_of_ways; x++) {
		std::cout<<name()<<"_";
		printf("way[%d]..", x);
		printf("state=%d..", m_cache_lines[set][x].state);
		printf("tag=0x%08x..", m_cache_lines[set][x].tag);
		printf("lru=%d\r\n", m_cache_lines[set][x].evict_tag);
	}
}




void cache::set_parent(cache *parent) {
	m_parent = parent;
}

void cache::set_children(std::vector< cache * > *child) {
	m_child = child;
}

void cache::set_delays(sc_core::sc_time upstream, sc_core::sc_time downstream, sc_core::sc_time lookup, sc_core::sc_time write, sc_core::sc_time read) {
	m_lookup_delay = lookup;
	m_write_cache_delay = write;
	m_read_cache_delay = read;
	m_upstream_cacheblock_delay = upstream;
	m_downstream_cacheblock_delay = downstream;
}


void cache::update_state(uint32_t operation, addr_t req_addr, sc_core::sc_time &delay) {
	addr_t addr = (req_addr/m_cache_line_size)*m_cache_line_size;			// rounded to cache-block address
	// finding out the cache set for this address for lookup
	uint32_t set = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE))) & ((1 << (uint32_t) log2((double)m_num_of_sets)) - 1);
	// as well as the tag
	addr_t tag = (addr >> (uint32_t) (log2((double) WORD_SIZE) + log2((double) m_cache_line_size/WORD_SIZE) + log2((double) m_num_of_sets)));

	bool debug = false;

	for (uint32_t i=0; i<m_num_of_ways; i++) {
		// finding the block to update its state
		if (m_cache_lines[set][i].tag == tag) {
			// operation = 1 => child wants to notify the parent that it is modifying the shared block...go recursively through all parents upto memory
			// operation = 2 => parent wants to notify the child for back invalidation...go recursively through all children upto first level cache

			switch(operation) {
			case 1:
				if (m_write_back) {
					m_cache_lines[set][i].state = cache_line::MBS;
				}
				// going over all of its parents
				if (m_parent) {
					m_parent->update_state(1, req_addr, delay);
				}
				break;
			case 2:
				if (m_write_back && m_cache_lines[set][i].state==cache_line::M) {
					// writeback this block to the cache/mem module above than the cache which initiated the back-invalidation
					// TODO: for now we just update the delay with a fixed amount no matter where the writeback is going to be done..later can use more accurate timing
					delay += WRITEBACK_DELAY;
				}
				m_cache_lines[set][i].state = cache_line::I;
				// replacement policy management stuff
				if (m_evict == LRU) {
					for (uint32_t j=0; j<m_num_of_ways; j++) {
						if ((uint32_t)i!=j && m_cache_lines[set][j].state!=cache_line::I && m_cache_lines[set][j].evict_tag>m_cache_lines[set][i].evict_tag) {
							m_cache_lines[set][j].evict_tag--;
						}
						assert(m_cache_lines[set][j].evict_tag <= m_num_of_ways);
					}
					m_cache_lines[set][i].evict_tag = m_num_of_ways;
				}
				// going over all of its childs
				if (m_child) {
					for (uint32_t j=0; j<m_child->size(); j++) {
						(*m_child)[j]->update_state(2, req_addr, delay);
					}
				}
				break;
			default:
				assert(0);
			}

			debug = true;
		}
		break;
	}

	assert(debug == true);		// there must be a block hit here!!

	// TODO: more accurate timing model for update state delay rather than a fixed value for every case
	delay += UPDATE_STATE_DELAY;
}






// TODO: FIFO replacement policy cache line
// TODO: review timing modeling in general

// TODO: deal with coherency issues
// sofar no support for cache coherence
// to avoid cache coherency problems in child caches sharing a common parent like split l1 (i/d) cache with unified l2 cache, the child caches (l1 i/d) should be made write through so that childs are not caching the same block with dirty state simultaneously with their own data









